# WEEKLY SPECIFICATION REPORT

## DESCRIPTION

**Topic:** First expression of needs by M. Ramel

**Objective:** Understanding the needs brief

**Author:** Th√©o Boisseau

**Creation date:** 22.09.2022

**Time spent:** 5h

**Version:** 1.1

## SUMMARY

1. [EXPRESSION OF NEEDS](#I.)
2. [ANALYSIS OF THE EXISTING SITUATION](#II.)
3. [FEASIBILITY ANALYSIS](#III.)


# I. EXPRESSION OF NEEDS <a id="I."></a>

CESR historians need interactive software to create their own scenarios for extracting content items from ancient document images.

The scenarios created could be reused and adapted for different books.
They would allow only the desired content elements to be extracted, such as dropped capital letter, banners, large letters, titles, paragraphs, words, print or handwritten character.


# II. ANALYSIS OF THE EXISTING SITUATION <a id="II."></a>

## SITUATION <a id="II.Situation"></a>

After the document image has been binarised by Agora, with the possibility to choose between different binarisation algorithms, the black pixels are processed by scenarios that aim to group together those that touch each other in order to label content items.

A set or subset (defined in a scenario) of the content items with their labels is extracted into an *ALTO* file which gives their position in the original image.
There is one file per image.

The file system is organised using:
- a hierarchical structure: \<"results" directory\>/\<one directory per content type\>/\<one subdirectory per image\>/\<thumbnail images\>
- an explicit naming convention to locate thumbnail by name, for example: 1.10.5.1.5 (\<page\>.\<paragraph\>.\<line\>.\<word\>.\<character\>), 1.1 (\<page\>.\<dropped capital letter\>)


### SCENARIOS

Among all the images in a document/collection, the expert (user) selects a typical one on which to build and test specific scenarios to extract pixel clusters, split them, merge them and label them.

Users can save their scenarios and run them on other collections.

Scenarios can lead to the labelling of pixel clusters but also to various operations on them that are defined by the user using rules.
They can be edited to be merged or split, and run iteratively.


### RULES

The rules used by the scenarios are either labelling rules or merger rules.

The labelling rules concern the position, size and neighbourhood of the content items, for example:
- if the content item has coordinates x < 121p, it is given the label of *margin note*.
- if the content item is located in the top 25% of the image, it is given the label of *title*.
- if the content item is larger than 50p, it is given the label of *dropped capital letter*.
- if the content item has a right neighboor, it is given the label of *dropped capital letter*.

The merger rules concern only the neighbourhood of the content items, for example:
- if the distance between two content items is less than 10p horizontally, they are merged.
- if two successive content items are [some item] and [some other item], they are merged.
- all content items in the top 121p are merged and are given the label of *title*.


## USE CASES

The software is typically used for 2 cases:
- to extract all content items of a document.
These items can then be used by the Retro OCR software to recognise characters.
- to extract only figures such as dropped capital letters and banners.
These figures can be stored in a database such as the CESR's Base de Typographie de la Renaissance (BaTyR) for renaissance typography, in order to study their history and thus that of their creators.


## TO CHANGE

Binarisation is problematic because it is not efficient enough.

In the case of handwritten documents, one or more characters, words or even lines tend to touch each other and are treated as the same content items.
Therefore, the extraction of handwritten content items is impossible because too many of them touch each other.

 <a id="II.ToChange.ALTO"></a>
Moreover, in ALTO, each ID must begin with a letter, not an integer.
The IDs which are used to build the file system in the last version of AGORA only use integers and this is not valid (see [here](#II.Situation)).


# III. FEASIBILITY ANALYSIS <a id="III."></a>

The objective is to build a deep learning module to better extract the content items.

In the first version, the module should replace the binarisation and extract the related components, representing content items.
It should take an image as input and return a list of related components.

The module should evolve in the next iterations, to better suit the two typical use cases presented above.

Ideally, the module should be able to extract printed and handwritten text.

Two ways will be possible:
- to create a generalist model and select only the subset of content item types defined by the user
- to create several models or an adaptive model that will offer different user-defined modules capable of extracting different subsets of content item types defined by the user

Note that the second way leaves open the idea of letting users create their own deep learning models.

In both cases, we can also reflect on the possibility of integrating scenarios into deep learning models.

